-module(ragno_fetcher).
-export([main/1]).

re_extract_links(Text) ->
    re:run(Text,
	   "<a href=\"(?P<A>[^\"]+)\"", 
	   [{capture,['A'],list}, global]).

is_http_link(Url) ->
    string:prefix(Url, "http") =/= nomatch.

extract_links(Text, BaseUrl) ->
    case re_extract_links(Text) of
	{match, List} ->
	    BinUrls = lists:map(fun(X) -> list_to_binary(X) end, 
				List),
	    Urls = lists:flatten(BinUrls),
	    HttpUrls = lists:filter(fun is_http_link/1, Urls),
	    AbsUrls = absolute_urls(HttpUrls, BaseUrl),
	    lists:filter(fun(X) -> is_binary(X) end, AbsUrls)
      ;
	_ -> []
    end.
     
absolute_urls(Urls, BaseUrl) ->
    lists:foldl(fun(X, Acc) -> case uri_string:resolve(X, BaseUrl) of
				   {error, _} -> Acc;
				   NewUrl -> [NewUrl|Acc]
			       end
		end,
		[],
		Urls).	 

base_url(Url) ->
    %%io:format("DEBUG: base_urll ~p\n", [Url]),
    UrlMap = uri_string:parse(Url),
    %% removing the fragment
    UrlMapNoQuery= maps:remove(query, UrlMap),
    UrlMapNoFragment = maps:remove(fragment, UrlMapNoQuery),
    %% removing the path
    uri_string:recompose(maps:put(path, "/", UrlMapNoFragment)).

base_urls(Urls) ->
    BaseUrls = lists:map(fun base_url/1, 
			 Urls),
    lists:usort(BaseUrls).
    
fetch_page(Url) ->
    httpc:request(get, 
		  {Url, [{"User-Agent", "Ragno/1.0"}]}, 
		  [{ssl, [{verify, verify_none}]},
		   {timeout, timer:seconds(8)},
		   {autoredirect, false}
		  ], 
		  [{body_format, binary}]).

fetch_page_with_manual_redirect(URL) ->
    case fetch_page(URL) of
	{ok, {{HttpVersion, Code, Reason}, Headers, Body}}  when Code >= 200, Code < 299  ->
	    {ok, URL, {{HttpVersion, Code, Reason}, Headers, Body}};
	{ok, {{_, Code, _}, Headers, _}}  when Code < 310 , Code >= 300 ->
	    NewURL=proplists:get_value("location", Headers),
	    fetch_page_with_manual_redirect(NewURL);
	{ok, {{HttpVersion, Code, Reason}, Headers, _}}  when Code >= 400 ->
	    {ok, URL, {{HttpVersion, Code, Reason}, Headers, ""}};
	Error -> Error
    end.

get_url_filename(Url) ->
    UrlMap = uri_string:parse(Url),
    Host = maps:get(host, UrlMap),
    List = re:split(Host, "\\.", [{return, list}]),
    [Dir1, [C2|_Dir2]|_] = lists:reverse(List),
    Dir = io_lib:format("data/~s/~s/", [Dir1, [C2]]),
    filelib:ensure_dir(Dir),
    Filename = re:replace(Url, "/", "", [{return, binary}, global]),
    io_lib:format("~s~s", [Dir, Filename]).
  
crawl_base_url(Url) when is_list(Url) -> 
    crawl_base_url(list_to_binary(Url));
crawl_base_url(Url) when is_binary(Url) ->
    io:format("DEBUG: crawling ~p\n", [Url]),
    case fetch_page_with_manual_redirect(Url) of
	{ok, FinalUrl, {_Resp, Headers, Body}} ->
	    Links = extract_links(Body, FinalUrl),
	    %%io:format("DEBUG: Links ~p\n", [Links]),
	    BaseLinks = base_urls(Links),
	    NormalizedLinks = lists:map(fun uri_string:normalize/1, 
					BaseLinks),
	    Filename = get_url_filename(Url),
	    FilenameDat = io_lib:format("~s.dat", [Filename]),
	    ok = file:write_file(FilenameDat, io_lib:format("[{url, ~p}, {final_url, ~p}, {headers, ~p}, {links, ~p}].", [Url, FinalUrl, Headers, Links])),
	    FilenameLinks = io_lib:format("~s.links", [Filename]),
	    file:write_file(FilenameLinks, string:join(NormalizedLinks, "\n")),
	    file:write_file(?VISITED_URLS, io_lib:format("~s\n", [Url]), [append])
      ;
	{error, Error} ->
	    %% something went wrong
	    io:format("ERROR: crawling ~p\n\n~p", [Url, Error]),
	    file:write_file(?SKIPPED_URLS, io_lib:format("~s\n", [Url]), [append])
    end.

crawl_base_urls(Urls) ->
    [spawn(fun() -> crawl_base_url(Url) end) || Url <- Urls],
    Sleep = max(length(Urls) * 600, 15000), 
    timer:sleep(Sleep),
    io:format("DEBUG: Bye!\n").

main([Url|Urls]) ->
    inets:start(),
    ssl:start(),
    %%application:ensure_all_started(ssl),
    %%httpc:set_options([{proxy, {{"proxy.local", 80}, 
    %%			["localhost"]}}]),
    crawl_base_urls([Url|Urls]);
main(_) ->
    usage().

usage() ->
    io:format("usage: https://sample.org\n"),
    halt(1).

